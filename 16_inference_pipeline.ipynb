{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/snehitha/taxi_project/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Move up one level and add \"src\" directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "\n",
    "# Now import the module\n",
    "import taxi_project.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:22:27,596 INFO: Initializing external client\n",
      "2025-03-05 09:22:27,597 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-05 09:22:28,582 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215691\n",
      "Fetching data from 2025-02-04 14:22:27.595162+00:00 to 2025-03-05 13:22:27.595162+00:00\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.73s) \n"
     ]
    }
   ],
   "source": [
    "from taxi_project.inference import get_feature_store\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd  \n",
    "\n",
    "# Get the current datetime64[us, Etc/UTC]  \n",
    "current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "feature_store = get_feature_store()\n",
    "\n",
    "# read time-series data from the feature store\n",
    "fetch_data_to = current_date - timedelta(hours=1)\n",
    "fetch_data_from = current_date - timedelta(days=1*29)\n",
    "print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "feature_view = feature_store.get_feature_view(\n",
    "    name=config.FEATURE_VIEW_NAME, version=config.FEATURE_VIEW_VERSION\n",
    ")\n",
    "\n",
    "ts_data = feature_view.get_batch_data(\n",
    "    start_time=(fetch_data_from - timedelta(days=1)),\n",
    "    end_time=(fetch_data_to + timedelta(days=1)),\n",
    ")\n",
    "ts_data = ts_data[ts_data.pickup_hour.between(fetch_data_from, fetch_data_to)]\n",
    "ts_data.sort_values([\"pickup_location_id\", \"pickup_hour\"]).reset_index(drop=True)\n",
    "ts_data[\"pickup_hour\"] = ts_data[\"pickup_hour\"].dt.tz_localize(None)\n",
    "\n",
    "from taxi_project.data_utils import transform_ts_data_info_features\n",
    "features = transform_ts_data_info_features(ts_data, window_size=24*28, step_size=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:22:38,511 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-05 09:22:38,514 INFO: Initializing external client\n",
      "2025-03-05 09:22:38,514 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-05 09:22:39,151 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215691\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "from taxi_project.inference import load_model_from_registry\n",
    "\n",
    "model = load_model_from_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:22:42,430 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-05 09:22:42,433 INFO: Initializing external client\n",
      "2025-03-05 09:22:42,434 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:22:43,094 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215691\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "from taxi_project.inference import load_model_from_registry\n",
    "\n",
    "model = load_model_from_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from taxi_project.inference import get_model_predictions\n",
    "predictions = get_model_predictions(model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>predicted_demand</th>\n",
       "      <th>pickup_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>255</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>237</td>\n",
       "      <td>388.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>186</td>\n",
       "      <td>173.0</td>\n",
       "      <td>2025-03-05 15:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pickup_location_id  predicted_demand               pickup_hour\n",
       "0                   255               3.0 2025-03-05 15:00:00+00:00\n",
       "1                   112               0.0 2025-03-05 15:00:00+00:00\n",
       "2                   218               0.0 2025-03-05 15:00:00+00:00\n",
       "3                   237             388.0 2025-03-05 15:00:00+00:00\n",
       "4                    57               0.0 2025-03-05 15:00:00+00:00\n",
       "..                  ...               ...                       ...\n",
       "246                 108               0.0 2025-03-05 15:00:00+00:00\n",
       "247                  71               0.0 2025-03-05 15:00:00+00:00\n",
       "248                   8               0.0 2025-03-05 15:00:00+00:00\n",
       "249                 168               1.0 2025-03-05 15:00:00+00:00\n",
       "250                 186             173.0 2025-03-05 15:00:00+00:00\n",
       "\n",
       "[251 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"pickup_hour\"] = current_date.ceil('h')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 09:22:46,127 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-05 09:22:46,130 INFO: Initializing external client\n",
      "2025-03-05 09:22:46,130 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-05 09:22:46,887 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1215691\n"
     ]
    }
   ],
   "source": [
    "from taxi_project.inference import get_feature_store\n",
    "\n",
    "feature_group = get_feature_store().get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTION,\n",
    "    version=1,\n",
    "    description=\"Predictions from LGBM Model\",\n",
    "    primary_key=[\"pickup_location_id\", \"pickup_hour\"],\n",
    "    event_time=\"pickup_hour\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns={\"predicted_demand\": \"prediction\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.rename(columns={\"prediction\": \"predicted_demand\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup_location_id                      int32\n",
      "predicted_demand                      float64\n",
      "pickup_hour           datetime64[us, Etc/UTC]\n",
      "dtype: object\n",
      "   pickup_location_id  predicted_demand               pickup_hour\n",
      "0                 255               3.0 2025-03-05 15:00:00+00:00\n",
      "1                 112               0.0 2025-03-05 15:00:00+00:00\n",
      "2                 218               0.0 2025-03-05 15:00:00+00:00\n",
      "3                 237             388.0 2025-03-05 15:00:00+00:00\n",
      "4                  57               0.0 2025-03-05 15:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "print(predictions.dtypes)\n",
    "print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureStoreException",
     "evalue": "Features are not compatible with Feature Group schema: \n - prediction (type: 'float') is missing from input dataframe.\n - predicted_demand (type: 'double') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hsfs/feature_group.py:2959\u001b[0m, in \u001b[0;36mFeatureGroup.insert\u001b[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2957\u001b[0m     write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffline_backfill_every_hr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offline_backfill_every_hr\n\u001b[0;32m-> 2959\u001b[0m job, ge_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_group_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2960\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_dataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2963\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msave_report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalidation_options\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mget_type()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m   2972\u001b[0m     \u001b[38;5;66;03m# Also, only compute statistics if stream is False.\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m     \u001b[38;5;66;03m# if True, the backfill job has not been triggered and the data has not been inserted (it's in Kafka)\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_statistics()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hsfs/core/feature_group_engine.py:188\u001b[0m, in \u001b[0;36mFeatureGroupEngine.insert\u001b[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_feature_group_metadata(\n\u001b[1;32m    184\u001b[0m         feature_group, dataframe_features, write_options\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# else, just verify that feature group schema matches user-provided dataframe\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_schema_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataframe_features\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# ge validation on python and non stream feature groups on spark\u001b[39;00m\n\u001b[1;32m    193\u001b[0m ge_report \u001b[38;5;241m=\u001b[39m feature_group\u001b[38;5;241m.\u001b[39m_great_expectation_engine\u001b[38;5;241m.\u001b[39mvalidate(\n\u001b[1;32m    194\u001b[0m     feature_group\u001b[38;5;241m=\u001b[39mfeature_group,\n\u001b[1;32m    195\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mfeature_dataframe,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     ge_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/hsfs/core/feature_group_base_engine.py:171\u001b[0m, in \u001b[0;36mFeatureGroupBaseEngine._verify_schema_compatibility\u001b[0;34m(self, feature_group_features, dataframe_features)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# raise exception if any errors were found.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(err) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures are not compatible with Feature Group schema: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m err])\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote that feature (or column) names are case insensitive and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaces are automatically replaced with underscores.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mFeatureStoreException\u001b[0m: Features are not compatible with Feature Group schema: \n - prediction (type: 'float') is missing from input dataframe.\n - predicted_demand (type: 'double') does not exist in feature group.\nNote that feature (or column) names are case insensitive and spaces are automatically replaced with underscores."
     ]
    }
   ],
   "source": [
    "feature_group.insert(predictions, write_options={\"wait_for_job\": False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (taxi_project)",
   "language": "python",
   "name": "taxi_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
